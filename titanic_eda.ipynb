{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic information of the Kaggle Titanic data set is widely understated in this forum. In this post, I'm going to show you how to easily capture demographic information hidden in the full data set (the join of the training set and test set). You may be concernted about data leakage issues that often raise in data competition and interested in the related [discussion](https://www.kaggle.com/c/titanic/discussion/41928#235524). For the rest of this kernel, I'll simply use the entire data set for my demonstration and leave the judgement for your own modeling.\n",
    "\n",
    "Here is a list of observations I'm going to point out with advanced tricks of Pandas in conjunction with NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Library we need \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Visualization Defaults\n",
    "mpl.style.use('ggplot')\n",
    "pylab.rcParams['figure.figsize'] = 12, 8\n",
    "sns.set_style('white')\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  891 non-null    int64  \n",
      " 1   Survived     891 non-null    int64  \n",
      " 2   Pclass       891 non-null    int64  \n",
      " 3   Name         891 non-null    object \n",
      " 4   Sex          891 non-null    object \n",
      " 5   Age          714 non-null    float64\n",
      " 6   SibSp        891 non-null    int64  \n",
      " 7   Parch        891 non-null    int64  \n",
      " 8   Ticket       891 non-null    object \n",
      " 9   Fare         891 non-null    float64\n",
      " 10  Cabin        204 non-null    object \n",
      " 11  Embarked     889 non-null    object \n",
      "dtypes: float64(2), int64(5), object(5)\n",
      "memory usage: 83.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Replace the following two directories with those in the following comments\n",
    "df_train = pd.read_csv(r'F:\\GitHubData\\Titanic\\train.csv') # r\"../input/train.csv\"\n",
    "df_test = pd.read_csv(r'F:\\GitHubData\\Titanic\\test.csv') # r\"../input/test.csv\"\n",
    "df_all = pd.concat([df_train, df_test], join='outer', axis=0)\n",
    "df_train.name = 'Training data'\n",
    "df_test.name = 'Test data'\n",
    "\n",
    "# We have 11 features and 1 target variables\n",
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training Examples: 891\n",
      "Number of Test Examples = 418\n",
      "Shape of Training Examples = (891, 12)\n",
      "Shape of Test Examples = (418, 11)\n"
     ]
    }
   ],
   "source": [
    "# Dataset Dimensions\n",
    "print(f'Number of Training Examples: {df_train.shape[0]}')\n",
    "print(f'Number of Test Examples = {df_test.shape[0]}')\n",
    "print(f'Shape of Training Examples = {df_train.shape}')\n",
    "print(f'Shape of Test Examples = {df_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Survived', 'Ticket']\n",
      "['Age', 'Cabin', 'Embarked', 'Fare', 'Name', 'Parch', 'PassengerId', 'Pclass', 'Sex', 'SibSp', 'Ticket']\n"
     ]
    }
   ],
   "source": [
    "# Column name we have\n",
    "print(sorted(df_train.columns.tolist())) \n",
    "print(sorted(df_test.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             count        mean         std   min       25%       50%    75%  \\\n",
      "PassengerId  891.0  446.000000  257.353842  1.00  223.5000  446.0000  668.5   \n",
      "Survived     891.0    0.383838    0.486592  0.00    0.0000    0.0000    1.0   \n",
      "Pclass       891.0    2.308642    0.836071  1.00    2.0000    3.0000    3.0   \n",
      "Age          714.0   29.699118   14.526497  0.42   20.1250   28.0000   38.0   \n",
      "SibSp        891.0    0.523008    1.102743  0.00    0.0000    0.0000    1.0   \n",
      "Parch        891.0    0.381594    0.806057  0.00    0.0000    0.0000    0.0   \n",
      "Fare         891.0   32.204208   49.693429  0.00    7.9104   14.4542   31.0   \n",
      "\n",
      "                  max  \n",
      "PassengerId  891.0000  \n",
      "Survived       1.0000  \n",
      "Pclass         3.0000  \n",
      "Age           80.0000  \n",
      "SibSp          8.0000  \n",
      "Parch          6.0000  \n",
      "Fare         512.3292  \n",
      "------------------------------\n",
      "         count unique                        top freq\n",
      "Name       891    891  Smith, Miss. Marion Elsie    1\n",
      "Sex        891      2                       male  577\n",
      "Ticket     891    681                   CA. 2343    7\n",
      "Cabin      204    147                C23 C25 C27    4\n",
      "Embarked   889      3                          S  644\n"
     ]
    }
   ],
   "source": [
    "# Numeric variables in training set\n",
    "print(df_train.describe(include=[np.number]).T)\n",
    "print('-' * 30)\n",
    "# Categorical variables in training set\n",
    "print(df_train.describe(include=['O']).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data\n",
      "PassengerId column missing values: 0\n",
      "Survived column missing values: 0\n",
      "Pclass column missing values: 0\n",
      "Name column missing values: 0\n",
      "Sex column missing values: 0\n",
      "Age column missing values: 177\n",
      "SibSp column missing values: 0\n",
      "Parch column missing values: 0\n",
      "Ticket column missing values: 0\n",
      "Fare column missing values: 0\n",
      "Cabin column missing values: 687\n",
      "Embarked column missing values: 2\n",
      "------------------------------\n",
      "Test data\n",
      "PassengerId column missing values: 0\n",
      "Pclass column missing values: 0\n",
      "Name column missing values: 0\n",
      "Sex column missing values: 0\n",
      "Age column missing values: 86\n",
      "SibSp column missing values: 0\n",
      "Parch column missing values: 0\n",
      "Ticket column missing values: 0\n",
      "Fare column missing values: 1\n",
      "Cabin column missing values: 327\n",
      "Embarked column missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Missing values\n",
    "def displayMissing(df):\n",
    "    for col in df.columns.tolist():\n",
    "        print(f'{col} column missing values: {df[col].isnull().sum()}')\n",
    "\n",
    "for i, df in enumerate([df_train, df_test]):\n",
    "    print(f'{df.name}')\n",
    "    displayMissing(df)\n",
    "    if i == 0: print('-' * 30) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As pointed out in many exploratory data analysis on Kaggle Titanic data set, we have missing values in continuous features `Age` and `Fare` and categorical features `Embarked` and `Cabin`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# todo:  Ticket combination is the feature without any missing values.\n",
    "# We should try to extract any information from it\n",
    "# although it appears useless at the first glance.\n",
    "def getTicketPrefixAndNumber(df, col):\n",
    "    # naming the columns to be created\n",
    "    col_num = col + '_num'\n",
    "    col_alp = col + '_alp'\n",
    "\n",
    "    # get the last group of contiguous digits\n",
    "    df[col_num] = df[col].str.extract(r'(\\d+)$')\n",
    "    df[col_num].fillna(-1, inplace=True)\n",
    "\n",
    "    # get the entire string before a space followed by the last digit group\n",
    "    df[col_alp] = df[col].str.extract(r'(.*)\\ \\d+$')\n",
    "        #.replace({'\\.': '', '/': ''}, regex=True)\n",
    "    df[col_alp].fillna('M', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getTicketPrefixAndNumber(df_all, 'Ticket')\n",
    "df_all['Ticket_num'] = pd.to_numeric(df_all['Ticket_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are there any people sharing the same ticket number but come from different team? Yes.\n",
    "#So, throughout the EDA, we should use 'Ticket', instead of 'Ticket_num' only, as the groupby key\n",
    "islice = df_all.groupby('Ticket_num')['Ticket_alp'].transform(lambda x: x.nunique() > 1)\n",
    "df_all.loc[islice,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: after extracting information, we'll discard the ticket_num, ticket_alp columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if the extraction works as expected\n",
    "colnames = ['Ticket' + s for s in ['', '_num', '_alp']]\n",
    "df_all[colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survival rate varies across ticket number prefix; it can be a predictor, right?\n",
    "gtb1 = df_all[['Survived', 'Ticket']].groupby(['Ticket'])\n",
    "temp = (gtb1['Survived'].sum() / gtb1['Survived'].count() * 100).sort_values()\n",
    "temp.name = 'TeamSurvivalRate'\n",
    "df_all = pd.merge(df_all, temp, on='Ticket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "#Check one of the family of size larger than 5\n",
    "df_all.loc[df_all.groupby('Ticket')['Age'].transform('count') > 5, :]['Ticket'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: compute the credibility of the estimated survival rate for each travel team\n",
    "# The credibility is measure in terms of the proportion of valid `survived` data.\n",
    "def getCredibilitySurvivalRate(df):\n",
    "    # To get the length of the column, don't use `count()`\n",
    "    df['SRcredibility'] = pd.notnull(df['Survived']).sum() / df['Age'].shape[0]\n",
    "    return df\n",
    "df_all = df_all.groupby('Ticket').apply(getCredibilitySurvivalRate)\n",
    "df_all.loc[df_all['Ticket'] == 'PC 17608',:]\n",
    "# By the way, compute the size of each travel team\n",
    "df_all['TeamSize'] = df_all.groupby('Ticket')['Age'].transform(lambda x: x.shape[0])\n",
    "# The correct size of family\n",
    "df_all['FamilySize'] = df_all['SibSp'] + df_all['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# So it appears team size is a useful predictor too.\n",
    "print(df_all.groupby(['TeamSize','Ticket'])['TeamSurvivalRate'].\\\n",
    "    apply(lambda x: x.mean()).groupby(level=0).mean())\n",
    "print(df_all.groupby(['Pclass', 'TeamSize'])['Survived'].mean())\n",
    "print(df_all.groupby(['Pclass', 'TeamSize'])['Survived'].mean().\n",
    "    reset_index().sort_values(by='Survived'))\n",
    "\n",
    "islice = (df_all['Pclass'] == 3)  & (df_all['TeamSize'] == 11)\n",
    "df_all.loc[islice,:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: who is a child?\n",
    "# create a new column `Role`\n",
    "def getRole(df, cutoff=7):\n",
    "    df['Role'] = 'Man'\n",
    "    df.loc[df['Sex'] == 'female', 'Role'] = 'Woman'\n",
    "    df.loc[df['Age'] <= cutoff, 'Role'] = 'Child'\n",
    "    return df\n",
    "# I want to hyper tune the age limit for the definition of a young child.\n",
    "# So I manually search over the integer sequence of [1,29]\n",
    "# and compute the corresponding survival rates of three types of role.\n",
    "ans = []\n",
    "ages = range(1, 30)\n",
    "for cut in ages:\n",
    "    getRole(df_all, cutoff=cut)\n",
    "    g = df_all.groupby(['Role'])\n",
    "    # I covert the resulting Pandas series to a data frame object and then append\n",
    "    # it to the `ans` list object.\n",
    "    ans.append((g['Survived'].sum() / g['Survived'].count()).to_frame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To concatnate the data frames stored in the `ans` list object.\n",
    "temp = pd.concat(ans, axis=1)  # 3 by N (=len(ages))\n",
    "tb1 = pd.DataFrame(np.array(temp).T,\n",
    "                   columns=['Child', 'Man', 'Woman'], index=ages)  # N by 3\n",
    "tb1.index.name = 'Age'\n",
    "# Gonna melt the table tb1 for drawing a line plot\n",
    "tb1.reset_index(inplace=True)  # prep for melt\n",
    "tb2 = pd.melt(tb1, id_vars=['Age'], value_vars=['Child', 'Man', 'Woman'],\n",
    "              var_name='Role', value_name='Survival')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seaborn FacetGrid:\n",
    "# [link](https://seaborn.pydata.org/generated/seaborn.FacetGrid.html)\n",
    "g = sns.FacetGrid(tb2, col='Role', margin_titles=True)\n",
    "g = g.map(plt.plot, 'Age', 'Survival')\n",
    "# add vertical line\n",
    "axes = g.fig.axes\n",
    "for ax in axes:\n",
    "    ax.vlines(x=15, ymax=1, ymin=0, linestyles='dashed', alpha=0.3, colors='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily set cutoffChildAge = 15 (60% survival rates)\n",
    "getRole(df_all, cutoff=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To have a sense of the distribution of TeamSize\n",
    "df_all.groupby(['Ticket']).first()['TeamSize'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: are SibSp values unique within groups?\n",
    "# Use this as an example: df_all.loc[df_all['Ticket_num']==17608,:]\n",
    "# Sanity check to see if children in the same travel team share the same SibSp value.\n",
    "# step 1. create a new column for children SibSp value only.\n",
    "df_all['childSibSp'] = np.where(df_all['Role'] == 'Child', df_all['SibSp'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2. is the childSibSp value unique within each travel group? Yes.\n",
    "df_all.groupby('Ticket')['childSibSp'].nunique().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, We use childSibSp to identify siblings that is not covered by the 'Child'\n",
    "definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3. Broadcasting: in a group, let every entry can see the shared 'childSibSp' value.\n",
    "df_all['childSibSp'] = df_all.groupby('Ticket')['childSibSp'].transform('max')\n",
    "# 'cz otherwise it's 0 by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4. If an example's SibSp equals to the shared value, the example must\n",
    "# from a elder child of age greater than the age limit for the child definition.\n",
    "logic = (df_all['SibSp'] != 0) & \\\n",
    "        (df_all['SibSp'] == df_all['childSibSp']) &\\\n",
    "        (df_all['Role'] != 'Child')\n",
    "df_all.loc[logic, 'Role'] = 'olderChild'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# todo: who is the female household head and/or the male household head?\n",
    "# Step 1. Identify the FamilySize value of the youngest child in each travel team.\n",
    "df_all['childFamilySize'] = np.where(df_all['Role'].isin(['Child', 'olderChild']),\n",
    "                                     df_all['FamilySize'], 0)\n",
    "# Step 2. Broadcasting the FamilySize value of the youngest child to\n",
    "# other members in the same travel team\n",
    "df_all['childFamilySize'] = df_all.groupby('Ticket')['childFamilySize'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Step 3. identy people who are parents\n",
    "def isMotherOrFather(s):\n",
    "    return 'Father' if s == 'Man' else 'Mother'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_logic = ((df_all['FamilySize'] == df_all['childFamilySize']) & \\\n",
    "               (~df_all['Role'].isin(['Child', 'olderChild'])) & \\\n",
    "               (df_all['FamilySize'] > 0))\n",
    "# a trick to obtain the index of valid examples after logical operations\n",
    "slice_index = df_all.loc[slice_logic, :].index\n",
    "df_all.loc[slice_index, 'Role'] = \\\n",
    "    df_all.loc[slice_logic, :]['Role'].apply(isMotherOrFather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['ChildWAdult'] = 'Not Applicable'\n",
    "logic = (df_all['Role'].isin(['Child', 'olderChild']))\n",
    "df_all.loc[logic, 'ChildWAdult'] = np.where(\n",
    "    df_all.loc[logic, 'FamilySize'] > df_all.loc[logic, 'childSibSp'] + 1,\n",
    "    'Yes',\n",
    "    'No'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: How many children in this group?\n",
    "#Method 1.\n",
    "df_all['NumChild'] = df_all.groupby('Ticket')['Role'].\\\n",
    "    transform(lambda x: x.isin(['Child', 'olderChild']).sum())\n",
    "df_all['NumYoungChild'] = df_all.groupby('Ticket')['Role'].\\\n",
    "    transform(lambda x: x.isin(['Child']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Method 2.\n",
    "numChildDict = df_all.groupby('Ticket')['Age']\\\n",
    "    .apply(lambda x: (x <= cutoffChildAge).sum()).reset_index(name='NumChild')\n",
    "df_all.join(numChildDict, on='Ticket')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Although the survival rate by number of child varies, the highest survival rate\n",
    "# falls in family of three children and it holds across classes.\n",
    "print(df_all.groupby(['Pclass', 'NumChild'])['Survived'].mean())\n",
    "# So, number of children should be another good predictor;\n",
    "# In passenger classes 1 and 2, families with three children have higher\n",
    "# estimated survival rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: travel with dad, mom, or both? Is with mother better than with father? NO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works\n",
    "Note: if you move ['Role'] inside transform's lambda function, it fails.\n",
    "Below I compare two different approaches for completing similar logical\n",
    "operation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. The usual method for broadcasting simple logical operations.\n",
    "df_all['hasMother'] = df_all.groupby('Ticket')['Role'].\\\n",
    "    transform(lambda x: (x == 'Mother').sum() > 0)\n",
    "df_all['hasFather'] = df_all.groupby('Ticket')['Role'].\\\n",
    "    transform(lambda x: (x == 'Father').sum() > 0)\n",
    "df_all['hasBothParents'] = np.where(\n",
    "    df_all['hasMother'] & df_all['hasFather'],\n",
    "    True,\n",
    "    False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Advanced Method: more complicated logical operations\n",
    "logics = [(df_all['hasFather'] & ~df_all['hasMother'], 'with Father'),\n",
    "          (df_all['hasFather'] & df_all['hasMother'], 'with Both'),\n",
    "          (~df_all['hasFather'] & df_all['hasMother'], 'with Mother'),\n",
    "          (~df_all['hasFather'] & ~df_all['hasMother'], 'without Parents')]\n",
    "for logic, s in logics:\n",
    "    ans = df_all.loc[df_all['Age'] <= 10, :].loc[logic, :].\\\n",
    "        groupby('Ticket')['Survived'].mean().mean()\n",
    "    print(f\"{s:>15} {ans: .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Sanity check for only father groups\n",
    "index = df_all.loc[df_all['hasFather'],:].index\n",
    "df_all.loc[index,:].groupby('Ticket_num').groups.keys()\n",
    "df_all.loc[df_all['Ticket_num']==2079,:]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# See this link: https://medium.com/@ODSC/creating-if-elseif-else-variables-in-python-pandas-7900f512f0e4\n",
    "\n",
    "#Method 1. Not exactly what I want; it only identify the entry of mother, but\n",
    "#I want the result to be broadcasted to everyone in the same group.\n",
    "role = 'Mother'\n",
    "#Step 1. Identify a mother's life status\n",
    "conditions = [(df_all['Role'] != role), # 'Not applicable'\n",
    "              (df_all['Survived'].isnull()), # 'Unknown'\n",
    "              (df_all['Survived'] == 1.0), # 'Yes'\n",
    "              (df_all['Survived'] == 0.0) # 'No'\n",
    "              ]\n",
    "choices = ['Not applicable', 'Unknown', 'Yes', 'No']\n",
    "df_all['isMotherSurvived'] = np.select(conditions, choices)\n",
    "len(df_all[df_all['Role']=='Mother']['isMotherSurvived'].values) # 44 mothers on board\n",
    "\n",
    "#Step 2. \"Braodcast\" the group-specific result to other group members\n",
    "#Method 1. Use map + dictionary\n",
    "index = df_all[df_all['Role']=='Mother']['Ticket_num'] # step 1-1. get the index\n",
    "ss = pd.Series(df_all[df_all['Role']=='Mother']['isMotherSurvived'].values, index=index) # step 1-2. get the value\n",
    "df_all['isMotherSurvived'] = df_all['Ticket_num'].map(ss.to_dict()).fillna('not applicable') # step 1-3. use dictionary to update the rest\n",
    "\n",
    "#Method 2.\n",
    "#https://stackoverflow.com/questions/56708924/broadcast-value-to-dataframe-group-by-condition\n",
    "df_all['test3'] = df_all['isMotherSurvived'].where(df_all['Role'].eq('Mother'))\\\n",
    "    .groupby(df_all['Ticket_num']).transform('first').fillna('not applicable')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compact version based on the preceding uncommented methods 1 and 2.\n",
    "So, we can do it automatically based on the experimental code block above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "role = 'Mother'\n",
    "def isSurvived(df_all, role='Mother'):\n",
    "    # Step 1. Identify a mother's life status\n",
    "    conditions = [(df_all['Role'] != role),  # 'Not applicable'\n",
    "                  (df_all['Survived'].isnull()),  # 'Unknown'\n",
    "                  (df_all['Survived'] == 1.0),  # 'Yes'\n",
    "                  (df_all['Survived'] == 0.0)  # 'No'\n",
    "                  ]\n",
    "    choices = ['Not applicable', 'Unknown', 'Yes', 'No']\n",
    "    s = 'is' + role + 'Survived'\n",
    "    df_all[s] = np.select(conditions, choices)\n",
    "    df_all[s] = df_all[s].where(df_all['Role'].eq(role)) \\\n",
    "        .groupby(df_all['Ticket_num']).transform('first').fillna('not applicable')\n",
    "    return df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "roles = ['Mother', 'Father']\n",
    "for role in roles:\n",
    "    isSurvived(df_all, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: get the prefix of Cabin feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def getCabinPrefix(df):\n",
    "    # 'M' is assigned to missing values\n",
    "    df['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "getCabinPrefix(df_all)\n",
    "df_all['Deck'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# def imputeCabinePrefix(df_all):\n",
    "df_all['Deck'].value_counts()\n",
    "\n",
    "# Check to see if the 2nd half of the combined table are all NaN Survived data\n",
    "# (1) iloc works with slicing that includes right endpoint.\n",
    "# (2) iloc works with index only, so even though I need 'Survived, I use it separately.\n",
    "# (3) isnull() to see if there is any missing value\n",
    "df_all.iloc[df_train.shape[0]:, ]['Survived'].isnull().all()\n",
    "\n",
    "# Cabin numbers have clusters\n",
    "df_all['Cabin'].value_counts()\n",
    "# For example, 'B57 B59 B63 B66' corresponds to five persons\n",
    "# in the Ryerson family. People in the same cabin share the same\n",
    "# Ticket_alp and Ticket_num. These three variables should be highly\n",
    "# correlated.\n",
    "df_all.loc[df_all['Cabin'] == 'B57 B59 B63 B66']\n",
    "# 'B57 B59 B63 B66' maps to Ticket_alp = 'PC', which is a much larger group.\n",
    "df_all.loc[df_all['Ticket_alp'] == 'PC']['Survived'].sum()\n",
    "\n",
    "# We may check later whether each group can be identified or associated with higher servival rate\n",
    "# We may also check to see if couples have higher survival rates\n",
    "# Check Family Ryerson. The number of SibSp and Parch might have more information.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: extract names and titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def getLastNameAndTitle(df):\n",
    "    # (1) https://docs.python.org/2/library/re.html\n",
    "    # (2) Why this patterns works? See the [reason](https://shorturl.at/uAEM8).\n",
    "    # (3) This pattern works as well r'^([^,]*)'\n",
    "    # See the reference [link](https://shorturl.at/dwJMS)\n",
    "    df['LastName'] = df['Name'].str.extract(r'^(.+?),')\n",
    "    df['Title'] = df['Name'].str.split(', ', expand=True)[1].\\\n",
    "        str.split('.', expand=True)[0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getLastNameAndTitle(df_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "cols = ['Name', 'Title', 'LastName']\n",
    "# df_all[cols] works as well\n",
    "# colon cannot be ignored in df_all.loc[:,cols]\n",
    "df_all.loc[:, cols]\n",
    "\n",
    "# finding: People with the same surname may come from different families, for example,\n",
    "# check the group of surname 'Davies' we found Ticket #48871 corresponds\n",
    "# to three young men; ticket #33112 corresponds to one women of age 48 and\n",
    "# and a child with the same surname of age 8. However, an issue is found\n",
    "# that just using LastName is not sufficient to locate people of the same\n",
    "# family. For example, the record of the woman with Ticket #33112 shows\n",
    "# she comes with her two children. By slicing with Ticket #33112, we found the\n",
    "# woman indeed has two children whose surnames are different. So, we should only\n",
    "# use Ticket_num instead of LastName to identify people traveling together.\n",
    "df_all.loc[df_all['LastName'] == 'Davies', :].sort_values(by=['Ticket_num'])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: sanity check: Is it possible to have two Mrs in a travelling group? NO.\n",
    "Trick#1: conditinal count after groupby\n",
    "The answer is no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby('Ticket')['Ticket'].\\\n",
    "    apply(lambda x: (x == 'Mrs').sum()).reset_index(name='MrsCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Example of displaying group results\n",
    "gs = df_all.groupby('Ticket_num')\n",
    "print(gs.indices)  # dict\n",
    "\n",
    "# Method 1. Peek the grouped data by sampling; so only part of the data\n",
    "# grouped.groups.keys()\n",
    "# grouped.groups.items()\n",
    "# grouped.get_group(gpkey)\n",
    "import random\n",
    "sampled_group_key = random.sample(gs.groups.keys(), 100)\n",
    "group_list = list(map(lambda gpkey: gs.get_group(gpkey), sampled_group_key))\n",
    "for i, g in enumerate(group_list):\n",
    "    if len(g) > 3:\n",
    "        print(g)\n",
    "        break\n",
    "\n",
    "# Method 2. scan through the groups\n",
    "for i, g in gs.groups.items():\n",
    "    if len(g) > 4:\n",
    "        print(gs.get_group(i))\n",
    "        break\n",
    "        \n",
    "# We found: the record of the Christy indicates there are two children but only one shown.\n",
    "df_all.loc[df_all['LastName'] == 'Christy', :]        \n",
    "\n",
    "\n",
    "#Broadcasting\n",
    "#Identify travelling groups with children among which who are parents?\n",
    "# \n",
    "def addMaxParchMinSibSp(grp):\n",
    "    return pd.Series(\n",
    "        [grp['Parch'].max(), grp['SibSp'].min()],\n",
    "        ['maxParch', 'minSibSp']\n",
    "    )\n",
    "# JOIN versue MERGE [link](https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html)\n",
    "df_all = df_all.join(df_all.groupby('Ticket_num').apply(addMaxParchMinSibSp), on='Ticket_num')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# LOGICAL OPERATIONS -----------------------------------------------------------\n",
    "\n",
    "# Method 1. dataframe.where()\n",
    "# df_all['isMother'] = False\n",
    "# df_all['isMother'] = df_all['isMother'].\\\n",
    "    where((df_all['Title'] != 'Mrs') | (df_all['hasMaster'] != True), True)\n",
    "\n",
    "# Method 2. np.where()\n",
    "df_all['MotherWithMaster'] = np.where(\n",
    "    (df_all['Title'] == 'Mrs') & (df_all['hasMaster'] == True), \n",
    "    True, \n",
    "    False\n",
    ")\n",
    "df_all.loc[df_all['Title'] == 'Master', :][['Age', 'Survived']].mean()  # 5.48, 57%\n",
    "\n",
    "# Method 3. np.logical_and()\n",
    "def MWM(df):\n",
    "    return df.apply(lambda x: 1 if \n",
    "        np.logical_and(x['Title'] == 'Mrs', x['Sex'] == 'female') \n",
    "        else 0, axis=1)\n",
    "df_all['test'] = MWM(df_all)\n",
    "df_all.head(10)\n",
    "\n",
    "# BROADCASTING OPERATIONS -----------------------------------------------------------\n",
    "# Identify teams travelling with mother and children\n",
    "\n",
    "# Method 1. use `transform`\n",
    "\n",
    "temp1 = df_all.groupby(['Ticket'])['Title'].\\\n",
    "    transform(lambda x: x.eq('Master').any())\n",
    "temp2 = df_all.groupby(['Ticket'])['MotherWithMaster'].\\\n",
    "    transform(lambda x: x.eq(True).any())\n",
    "df_all['GroupWMomChild'] = temp1 & temp2\n",
    "\n",
    "# Method 2-A. use apply-turned `dictionary` and `map` IT WORKS!!\n",
    "\n",
    "# temp5 = df_all.groupby('Ticket').apply(\n",
    "#       lambda x: x['Title'].eq('Master').any() & x['MotherWithMaster'].eq(True).any())\n",
    "# df_all['GroupWMomChild_3'] = df_all['Ticket'].map(temp5)\n",
    "\n",
    "# Method 2-B. use apply and merge IT WORKS!!\n",
    "\n",
    "# temp3 = df_all.groupby(['Ticket']).apply(lambda x: x['Title'].eq('Master').any())\n",
    "# temp4 = df_all.groupby(['Ticket']).apply(lambda x: x['MotherWithMaster'].eq(True).any())\n",
    "# df_all.merge((temp3 & temp4).reset_index(), how='left').rename(columns={0: 'GroupWMomChild_2'})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: impute missing EMBARKED values using K nearest neighbors algorithm\n",
    "# They more likely board on the ship at port S -- Theory 1.\n",
    "df_all.loc[df_all['Embarked'].isnull(),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_all.loc[df_all['Ticket_num'].between(100000, 125000)]['Embarked'].value_counts())  # S\n",
    "print(df_all.loc[df_all['Fare'].between(60, 100)]['Embarked'].value_counts())  # S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's hard to tell which port they boarded from only based on their `Pclass` and `Fare` features.\n",
    "df_all.groupby(['Pclass',pd.cut(df_all['Fare'],range(50,100,15))])['Embarked'].\\\n",
    "    apply(lambda x: x.value_counts().nlargest(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I decide to make use of the information contained in the ticket combination:\n",
    "# Step 1. identifying data index corresponding to valid 'Embarked' data.\n",
    "index = df_all['Embarked'].isnull()\n",
    "# We are comfortable to only use three features to predict missing value.\n",
    "_dfAll = df_all.loc[:, ['Embarked', 'Pclass', 'Ticket_alp', 'Ticket_num']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. labeling and normalizing the feature matrix\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "encoder = LabelEncoder()\n",
    "minmax_scale = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2-A. encoding columns 'Pclass' and 'Ticket_alp'\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "# Note below that `fit_transform()` expects a 2D array, but encoder returns a 1D array\n",
    "# So we need to reshape it.\n",
    "for i in range(1,4):\n",
    "    temp = encoder.fit_transform(_dfAll.iloc[:,i]).reshape(-1,1)\n",
    "    _dfAll.iloc[:, i] = minmax_scale.fit_transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our feature matrix consists of `Pclass`, `Ticket_alp`, and `Ticket_num`.\n",
    "_xtrain = _dfAll.loc[~index,_dfAll.columns[1:4]]\n",
    "_ytrain = encoder.fit_transform(_dfAll.loc[~index, 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. prediction with k nearest neighbors algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "knc = KNeighborsClassifier(3, weights='distance')\n",
    "trained_knc = knc.fit(_xtrain, _ytrain)\n",
    "predicted_embarked_missing = trained_knc.predict(_dfAll.loc[index, _dfAll.columns[1:4]])\n",
    "# update the missing value with what we just obtained.\n",
    "df_all.loc[index,'Embarked'] = encoder.inverse_transform(predicted_embarked_missing) # S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: Fare () Imputation based on the size of the travel team and passneger class.\n",
    "#print(df_all.loc[df_all['Fare'].isnull(),:]) # index = 973\n",
    "islice = (df_all['Pclass'] == 3)\n",
    "sns.scatterplot(x='Age', y='Fare', size= 'TeamSize', data=df_all.loc[islice,:]); plt.show()\n",
    "# impute\n",
    "df_all['Fare'] = df_all.groupby(['Pclass','TeamSize'])['Fare']\\\n",
    "    .transform(lambda x: x.fillna(x.median()))\n",
    "print(df_all.iloc[973,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Ticekt_num is more useful than LastName (aka Family)\n",
    "1. even in the same family, when women are alive, men are not necessarily alive. Ticket#19950\n",
    "2. friends or colleauges are probably in the same ticket number group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Survival rate computation\n",
    "cols = df_all.columns\n",
    "# The Davies has two children and two adults (one is maid). The youngest child is alive.\n",
    "df_all.loc[df_all['Ticket_num'] == 33112, cols]\n",
    "df_all.loc[df_all['Ticket_num'] == 2079, cols]\n",
    "df_all.loc[df_all['Ticket_num'] == 36928, cols]  # old family\n",
    "# Just check out a few of groups of size 2\n",
    "# df_all.loc[df_all['TeamSize']==7,:]['Ticket_num'].unique()\n",
    "df_all.loc[df_all['Ticket_num'] == 236853, cols]  # size of 2\n",
    "df_all.loc[df_all['Ticket_num'] == 17608, cols]  # size of 6 (all the child die)\n",
    "df_all.loc[df_all['Ticket_num'] == 3101295, cols]\n",
    "df_all.loc[df_all['Ticket_num'] == 2144, cols]\n",
    "df_all.loc[df_all['Ticket_num'] == 347742, cols]\n",
    "df_all.loc[df_all['Ticket_num'] == 347077, cols]  # team with a mother that survived\n",
    "# It appears none of them are relatives except Lam Ali and Lam Len.\n",
    "df_all.loc[df_all['Ticket_num'] == 1601, cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ticket Number Distribution by Pclass and Embarked\n",
    "# The plot doesn't help to impute the two missing Embarked value, both of which are in Pclass = 1.\n",
    "# The only information gain is that given they share the same ticket number, they should know each\n",
    "# other and highly likely embark from either C or S together.\n",
    "g = sns.FacetGrid(df_train, col='Pclass', row='Embarked')\n",
    "g = g.map(sns.countplot, 'Ticket_num')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "g = sns.FacetGrid(df_all, col='Pclass', row='Sex')\n",
    "g = g.map(sns.countplot, 'Age')\n",
    "g.set(xticks=[range(10,70,10)])\n",
    "[plt.setp(ax.get_xticklabels(), rotation=90) for ax in g.axes.flat]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "##\n",
    "g = sns.FacetGrid(df_train, col='Pclass', row='Embarked', hue='Deck')\n",
    "g = g.map(plt.scatter, 'Age', 'Fare')\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Percentage of survived passengers by EMBARKED values -------------------------\n",
    "\n",
    "deck_embarked = df_train[['Deck', 'Embarked', 'Survived']].\\\n",
    "    groupby(['Deck', 'Embarked']).count()\n",
    "\n",
    "# solution 1\n",
    "tb1 = deck_embarked.groupby(level=1).apply(lambda x: 100 * x / float(x.sum()))\n",
    "tb1.rename(columns={'Survived': 'Passengers (%)'})\n",
    "\n",
    "# solution 2\n",
    "tb2 = 100 * deck_embarked / deck_embarked.groupby(level=1).transform('sum')\n",
    "tb2.rename(columns={'Survived': 'Passengers (%)'})\n",
    "\n",
    "# Group Selection Operation ----------------------------------------------------\n",
    "cols = ['Deck', 'Pclass']\n",
    "df_train.groupby(cols).filter(lambda x: x['Age'].\\\n",
    "    quantile(q=0.75) > 50)['Survived'].mean()\n",
    "df_train.groupby(cols).filter(lambda x: x['Age'].\\\n",
    "    quantile(q=0.75) < 30)['Survived'].mean()\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Add percentage bar number 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Add percentage bar number 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Plot training set survival distribution\n",
    "https://i.postimg.cc/25rVKwxB/1590377048.png\n",
    "https://python-graph-gallery.com/13-percent-stacked-barplot/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#Categorical variable plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Continuous variable plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "#Fare binning with qcut or cut"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
